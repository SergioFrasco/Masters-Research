Ours:
┌─────────────────────────────────────────────────────────┐
│                     AGENT STEP                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. Observe → Cube Detection → Ego View                │
│                                                         │
│  2. Vision Model (Autoencoder)                         │
│     Input: 13x13 ego view                              │
│     Output: Predicted reward locations                 │
│     ↓                                                   │
│     Build 10x10 global reward map R                    │
│                                                         │
│  3. Successor Representation Matrix M                  │
│     Size: (3 actions, 100 states, 100 states)         │
│     Updated via TD: M[a,s,:] += α(I + γM[a,s',:] - M[a,s,:])│
│     ↓                                                   │
│     Compute World Value Function:                      │
│     WVF = M[forward] @ R  (matrix multiply)            │
│     Shape: (100 states, 10, 10)                        │
│                                                         │
│  4. Action Selection                                    │
│     - Look at neighbor cells in WVF                    │
│     - Choose action toward highest value               │
│                                                         │
└─────────────────────────────────────────────────────────┘

WVF Baseline:
┌─────────────────────────────────────────────────────────┐
│                     AGENT STEP                          │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. Observe → Cube Detection → Ego View                │
│     [SAME AS BEFORE]                                   │
│                                                         │
│  2. Vision Model (Autoencoder)                         │
│     Input: 13x13 ego view                              │
│     Output: Predicted reward locations                 │
│     ↓                                                   │
│     Build 10x10 global reward map R                    │
│     [SAME AS BEFORE]                                   │
│                                                         │
│  3. WVF MLP (NEW!)                                     │
│     Input: [reward_map (10x10), agent_position (100)]  │
│            → Concatenated (200-dim)                    │
│     ↓                                                   │
│     Hidden: 200 → 256 → 256 → 256                     │
│     ↓                                                   │
│     Output: Q-values (10, 10, 3)                       │
│             = Q(s,a) for all state-action pairs       │
│     ↓                                                   │
│     Updated via TD: Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]│
│                                                         │
│  4. Action Selection                                    │
│     - Look at neighbor cells in Q-values               │
│     - Choose action toward highest value               │
│     [SAME LOGIC AS BEFORE]                             │
│                                                         │
└─────────────────────────────────────────────────────────┘