# -------------------------------
# DQN Vision-Based Agent
# -------------------------------
#
# This implementation defines a Deep Q-Network (DQN) agent that learns to navigate
# an environment using a 2D visual grid as input. The agent uses a convolutional
# neural network to process spatial layouts and determine optimal actions.
#
# âœ… Key Components:
# - Visual representation of state: walls, agent, goal.
# - DQN with experience replay and target network.
# - Convolutional layers for spatial feature learning.
#
# ðŸ§  Learning Process:
# - The agent stores experiences (state, action, reward, next_state, done).
# - It samples mini-batches from memory to train the Q-network.
# - Q-values are updated using the Bellman equation:
#     Q(s, a) <- r + Î³ * max_a' Q(s', a')
# - The target network is periodically updated to stabilize learning.
#
# ðŸ”„ Training Loop:
# - For each episode:
#     - Reset environment and observe initial visual state.
#     - Take actions using an Îµ-greedy policy.
#     - Collect transitions and train the Q-network.
#     - Update target network every N episodes.
# - Track score, steps, and Îµ-decay for exploration.
#
# ðŸŒ Policy Visualization:
# - A heatmap is generated by sweeping through all positions.
# - The model predicts Q-values from the visual state.
# - Max Q-values are visualized to indicate high-value regions.
#
# ðŸ“Š Evaluation:
# - Plots training performance (scores, steps).
# - Tests agent performance with greedy policy.
# - Visualizes learned Q-values across the grid.
#
# ðŸ“ƒ Logging & Utilities:
# - Model can be saved/loaded.
# - Performance stats are plotted.
# - Supports rendering during testing for inspection.
#
# Overall, this DQN agent demonstrates how spatially aware visual input can be
# leveraged to learn optimal control policies using deep reinforcement learning.


import os
import sys
import numpy as np
import tensorflow as tf
from tensorflow import keras
from collections import deque
import random
import matplotlib.pyplot as plt
from tqdm import tqdm

# Suppress TensorFlow logging
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
tf.config.set_visible_devices([], "GPU")


class DQNVisionAgent:
    """
    DQN Agent that uses visual representation of the environment
    """
    
    def __init__(self, env, state_shape=(10, 10, 1), action_size=3):
        self.env = env
        self.state_shape = state_shape
        self.action_size = action_size
        self.grid_size = env.size
        
        # DQN Hyperparameters
        self.memory = deque(maxlen=10000)  # Experience replay buffer
        self.epsilon = 1.0  # Exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.gamma = 0.95  # Discount factor
        self.batch_size = 32
        self.target_update_freq = 100  # Update target network every N episodes
        
        # Build neural networks
        self.q_network = self._build_model()
        self.target_network = self._build_model()
        self.update_target_network()
        
        # Training tracking
        self.training_step = 0
        self.episode_count = 0
        
    def _build_model(self):
        """
        Build the DQN model with convolutional layers for visual input
        """
        model = keras.Sequential([
            # Convolutional layers for spatial feature extraction
            keras.layers.Conv2D(32, (3, 3), activation='relu', 
                              input_shape=self.state_shape, padding='same'),
            keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
            keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
            
            # Flatten and fully connected layers
            keras.layers.Flatten(),
            keras.layers.Dense(512, activation='relu'),
            keras.layers.Dense(256, activation='relu'),
            keras.layers.Dense(self.action_size, activation='linear')
        ])
        
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss='mse'
        )
        
        return model
    
    def get_visual_state(self):
        """
        Convert environment to visual representation
        Returns 10x10 normalized image with:
        - Agent position: 1.0
        - Goal position: 0.8
        - Walls: -1.0
        - Empty space: 0.0
        """
        # Get environment grid
        grid = self.env.grid.encode()
        object_layer = grid[..., 0]
        
        # Initialize visual state
        visual_state = np.zeros((self.grid_size, self.grid_size), dtype=np.float32)
        
        # Set wall positions
        visual_state[object_layer == 2] = -1.0  # Wall
        
        # Set goal position
        visual_state[object_layer == 8] = 0.8   # Goal
        
        # Set agent position
        agent_x, agent_y = self.env.agent_pos
        visual_state[agent_y, agent_x] = 1.0    # Agent
        
        # Add channel dimension
        visual_state = visual_state[..., np.newaxis]
        
        return visual_state
    
    def remember(self, state, action, reward, next_state, done):
        """Store experience in replay buffer"""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        """Choose action using epsilon-greedy policy"""
        if np.random.random() <= self.epsilon:
            return np.random.choice(self.action_size)
        
        # Add batch dimension
        state_batch = state[np.newaxis, ...]
        q_values = self.q_network.predict(state_batch, verbose=0)
        return np.argmax(q_values[0])
    
    def replay(self):
        """Train the model on a batch of experiences"""
        if len(self.memory) < self.batch_size:
            return
        
        # Sample random batch
        batch = random.sample(self.memory, self.batch_size)
        
        # Extract batch components
        states = np.array([e[0] for e in batch])
        actions = np.array([e[1] for e in batch])
        rewards = np.array([e[2] for e in batch])
        next_states = np.array([e[3] for e in batch])
        dones = np.array([e[4] for e in batch])
        
        # Current Q values
        current_q_values = self.q_network.predict(states, verbose=0)
        
        # Next Q values from target network
        next_q_values = self.target_network.predict(next_states, verbose=0)
        
        # Calculate target Q values
        target_q_values = current_q_values.copy()
        
        for i in range(self.batch_size):
            if dones[i]:
                target_q_values[i][actions[i]] = rewards[i]
            else:
                target_q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])
        
        # Train the network
        self.q_network.fit(states, target_q_values, epochs=1, verbose=0)
        
        # Decay epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            
        self.training_step += 1
    
    def update_target_network(self):
        """Copy weights from main network to target network"""
        self.target_network.set_weights(self.q_network.get_weights())
    
    def save_model(self, filepath):
        """Save the trained model"""
        self.q_network.save(filepath)
    
    def load_model(self, filepath):
        """Load a trained model"""
        self.q_network = keras.models.load_model(filepath)
        self.update_target_network()


def train_dqn_agent(agent, env, episodes=2000, max_steps=200, verbose=True):
    """
    Train the DQN agent
    """
    scores = []
    step_counts = []
    losses = []
    
    for episode in tqdm(range(episodes), desc="Training DQN Agent"):
        # Reset environment
        obs = env.reset()
        state = agent.get_visual_state()
        
        total_reward = 0
        step_count = 0
        
        for step in range(max_steps):
            # Choose action
            action = agent.act(state)
            
            # Take action in environment
            next_obs, reward, done, _, _ = env.step(action)
            next_state = agent.get_visual_state()
            
            # Store experience
            agent.remember(state, action, reward, next_state, done)
            
            # Update state
            state = next_state
            total_reward += reward
            step_count += 1
            
            # Train the agent
            agent.replay()
            
            if done:
                break
        
        # Update target network periodically
        if episode % agent.target_update_freq == 0:
            agent.update_target_network()
        
        # Record statistics
        scores.append(total_reward)
        step_counts.append(step_count)
        agent.episode_count += 1
        
        # Print progress
        if verbose and episode % 100 == 0:
            avg_score = np.mean(scores[-100:])
            avg_steps = np.mean(step_counts[-100:])
            print(f"Episode {episode}: Avg Score={avg_score:.3f}, "
                  f"Avg Steps={avg_steps:.1f}, Epsilon={agent.epsilon:.3f}")
    
    return scores, step_counts


def test_dqn_agent(agent, env, episodes=100, max_steps=200, render=False):
    """
    Test the trained DQN agent
    """
    test_scores = []
    test_steps = []
    
    # Set epsilon to 0 for testing (no exploration)
    original_epsilon = agent.epsilon
    agent.epsilon = 0
    
    for episode in tqdm(range(episodes), desc="Testing DQN Agent"):
        obs = env.reset()
        state = agent.get_visual_state()
        
        total_reward = 0
        step_count = 0
        
        for step in range(max_steps):
            action = agent.act(state)
            obs, reward, done, _, _ = env.step(action)
            state = agent.get_visual_state()
            
            total_reward += reward
            step_count += 1
            
            if render and episode < 5:  # Render first few episodes
                env.render()
            
            if done:
                break
        
        test_scores.append(total_reward)
        test_steps.append(step_count)
    
    # Restore original epsilon
    agent.epsilon = original_epsilon
    
    return test_scores, test_steps


def plot_training_results(scores, step_counts, save_path=None):
    """
    Plot training results
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # Smooth the data
    window = 50
    scores_smooth = np.convolve(scores, np.ones(window)/window, mode='valid')
    steps_smooth = np.convolve(step_counts, np.ones(window)/window, mode='valid')
    
    # Plot scores
    axes[0, 0].plot(scores, alpha=0.3, label='Raw')
    axes[0, 0].plot(range(window-1, len(scores)), scores_smooth, 
                    linewidth=2, label='Smoothed')
    axes[0, 0].set_title('Episode Scores')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Score')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # Plot steps
    axes[0, 1].plot(step_counts, alpha=0.3, label='Raw')
    axes[0, 1].plot(range(window-1, len(step_counts)), steps_smooth, 
                    linewidth=2, label='Smoothed')
    axes[0, 1].set_title('Steps per Episode')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Steps')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # Plot score distribution
    axes[1, 0].hist(scores, bins=30, alpha=0.7, edgecolor='black')
    axes[1, 0].set_title('Score Distribution')
    axes[1, 0].set_xlabel('Score')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].axvline(np.mean(scores), color='red', linestyle='--', 
                      label=f'Mean: {np.mean(scores):.3f}')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # Plot steps distribution
    axes[1, 1].hist(step_counts, bins=30, alpha=0.7, edgecolor='black')
    axes[1, 1].set_title('Steps Distribution')
    axes[1, 1].set_xlabel('Steps')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].axvline(np.mean(step_counts), color='red', linestyle='--', 
                      label=f'Mean: {np.mean(step_counts):.1f}')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path)
        print(f"Plot saved to {save_path}")
    
    plt.show()


def visualize_agent_policy(agent, env, save_path=None):
    """
    Visualize the learned policy by showing Q-values for each position
    """
    grid_size = env.size
    
    # Create a grid to store max Q-values for each position
    q_value_grid = np.zeros((grid_size, grid_size))
    
    # Store original environment state
    original_pos = env.agent_pos
    original_dir = env.agent_dir
    
    # Test each position
    for y in range(grid_size):
        for x in range(grid_size):
            # Check if position is valid (not a wall)
            if env.grid.get(x, y) is None:
                # Temporarily place agent at this position
                env.agent_pos = (x, y)
                
                # Get visual state
                state = agent.get_visual_state()
                
                # Get Q-values
                q_values = agent.q_network.predict(state[np.newaxis, ...], verbose=0)
                
                # Store max Q-value
                q_value_grid[y, x] = np.max(q_values[0])
            else:
                # Wall or invalid position
                q_value_grid[y, x] = np.nan
    
    # Restore original state
    env.agent_pos = original_pos
    env.agent_dir = original_dir
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.imshow(q_value_grid, cmap='hot', interpolation='nearest')
    plt.colorbar(label='Max Q-Value')
    plt.title('Learned Policy Visualization (Max Q-Values)')
    plt.xlabel('X Position')
    plt.ylabel('Y Position')
    
    if save_path:
        plt.savefig(save_path)
        print(f"Policy visualization saved to {save_path}")
    
    plt.show()


def main():
    """
    Main function to train and test DQN agent
    """
    # Import environment
    from env import SimpleEnv

    # Create environment and agent
    env = SimpleEnv(size=10)
    
    # For demonstration
    print("DQN Vision Agent Implementation")
    print("=" * 50)
    
    # Create agent
    agent = DQNVisionAgent(env, state_shape=(10, 10, 1), action_size=3)
    
    # Training
    print("Training DQN Agent...")
    scores, step_counts = train_dqn_agent(agent, env, episodes=2000)
    
    # Save model
    # agent.save_model('dqn_vision_model.h5')
    
    # Test agent
    print("\nTesting trained agent...")
    test_scores, test_steps = test_dqn_agent(agent, env, episodes=100)
    
    # Print results
    print(f"Training - Average Score: {np.mean(scores[-100:]):.3f}")
    print(f"Training - Average Steps: {np.mean(step_counts[-100:]):.1f}")
    print(f"Testing - Average Score: {np.mean(test_scores):.3f}")
    print(f"Testing - Average Steps: {np.mean(test_steps):.1f}")
    
    # Plot results
    plot_training_results(scores, step_counts, 'dqn_training_results.png')
    
    # Visualize policy
    visualize_agent_policy(agent, env, 'dqn_policy_visualization.png')
    
    print("\nDQN implementation complete!")


if __name__ == "__main__":
    main()